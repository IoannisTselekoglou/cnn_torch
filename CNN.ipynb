{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eVTGHo7fNOdb"
   },
   "outputs": [],
   "source": [
    "#load data from files\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "from sklearn import preprocessing as pc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJlYqo6iMm7J",
    "outputId": "0b336a3d-f9a3-4aaa-e7b7-a5f43d3c8e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-vLdfX6tNXkp"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((150,150)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "labels_dic = {\"buildings\": 0, \"forest\": 1, \"glacier\" : 2, \"mountain\" : 3, \"sea\" : 4, \"street\" : 5}\n",
    "encoder = {0: \"buildings\", 1: \"forest\", 2 : \"glacier\", 3 : \"mountain\", 4: \"sea\", 5: \"street\"}\n",
    "\n",
    "def load_data(data):\n",
    "    images = []\n",
    "    classes = [\"building\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "    labels = []\n",
    "    for i in classes:\n",
    "        path = os.path.join((f\"./dataset/{data}\"),i)\n",
    "        path += \"/*.jpg\"\n",
    "        for file in glob.glob(path):\n",
    "            labels.append(labels_dic[i])\n",
    "            im = Image.open(file)\n",
    "            images.append(transform(im))\n",
    "            im.close()\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "#take big ass chucnk of data test_x\n",
    "#shuffle data\n",
    "train_set = load_data(\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fJJ4uVakOOX5"
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "size = 100\n",
    "shuffle_iters = len(train_set[0])/size\n",
    "\n",
    "def split_data(data_set): \n",
    "    pile = []\n",
    "    i = 0\n",
    "    while i < len(data_set):\n",
    "        if (len(data_set) - size) < size:\n",
    "            pile.append(data_set[i:i+len(data_set)])\n",
    "           #break\n",
    "        pile.append(data_set[i:i+size])\n",
    "        i += size\n",
    "    return pile\n",
    "\n",
    "trainset_x = split_data(train_set[0])\n",
    "trainset_y = split_data(train_set[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Heh6EtT7-nBX"
   },
   "outputs": [],
   "source": [
    "#shuffle data \n",
    "def shuffle_data(data_set):\n",
    "    size = int(len(data_set)/2)\n",
    "    result_data = []\n",
    "    counter_x = 0\n",
    "    counter_y = size\n",
    "    for i in range(size):\n",
    "        x = data_set[counter_x]\n",
    "        y = data_set[counter_y]\n",
    "        result_data.append(y)\n",
    "        result_data.append(x)\n",
    "        counter_x += 1\n",
    "        counter_y += 1\n",
    "    return result_data\n",
    "    \n",
    "trainset_x = shuffle_data(trainset_x)\n",
    "trainset_y = shuffle_data(trainset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_l9x8HS2-wfw"
   },
   "outputs": [],
   "source": [
    "def flatten_x(l):\n",
    "    data_x = []\n",
    "    for i in l:\n",
    "        data_x += i\n",
    "    return data_x\n",
    "    \n",
    "train_x = flatten_x(trainset_x)\n",
    "\n",
    "def flatten_y(l):\n",
    "    return [item for sublist in l for item in sublist] #stackoverflow provieds nicest list comprehensions \n",
    "\n",
    "train_y = flatten_y(trainset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k0WPVAFGNiUD"
   },
   "outputs": [],
   "source": [
    "class Mydata(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y): \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = len(x)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx] , self.y[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0M5VNVwGOEyW"
   },
   "outputs": [],
   "source": [
    "trainset = Mydata(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U2xcepP_OYv4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 6)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, 3)\n",
    "        self.conv4 = nn.Conv2d(24, 12, 3)\n",
    "        self.fc1 = nn.Linear(588, 244)\n",
    "        self.fc2 = nn.Linear(244, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = self.pool1(F.relu(self.conv3(x)))\n",
    "        x = self.pool1(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        #fully connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H8N6IvXYrCPD"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NYS7oR78C5l5"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_ngYbzSHaxp",
    "outputId": "5efed1f8-25d9-409e-dca5-634b30b1d448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----epoch:1-----\n",
      "\n",
      "running_loss: 1.7998161315917969\n",
      "running_loss: 1.589583158493042\n",
      "running_loss: 1.266858696937561\n",
      "running_loss: 1.0228931903839111\n",
      "running_loss: 1.009373426437378\n",
      "running_loss: 1.0201294422149658\n",
      "running_loss: 0.9540987610816956\n",
      "running_loss: 0.784099817276001\n",
      "running_loss: 1.0710543394088745\n",
      "running_loss: 1.0118101835250854\n",
      "running_loss: 0.8769830465316772\n",
      "running_loss: 0.8404551148414612\n",
      "\n",
      " loss_model: 0.9926261305809021\n",
      "\n",
      "-----epoch:2-----\n",
      "\n",
      "running_loss: 0.8442928194999695\n",
      "running_loss: 0.7919766306877136\n",
      "running_loss: 0.8320675492286682\n",
      "running_loss: 0.8478043079376221\n",
      "running_loss: 0.98211270570755\n",
      "running_loss: 0.9338610768318176\n",
      "running_loss: 0.8847571611404419\n",
      "running_loss: 0.8257501721382141\n",
      "running_loss: 0.7072792649269104\n",
      "running_loss: 0.7312979102134705\n",
      "running_loss: 0.6933773756027222\n",
      "running_loss: 0.7206282615661621\n",
      "\n",
      " loss_model: 0.8309004306793213\n",
      "\n",
      "-----epoch:3-----\n",
      "\n",
      "running_loss: 0.8585695624351501\n",
      "running_loss: 0.7151486873626709\n",
      "running_loss: 0.7426245808601379\n",
      "running_loss: 0.7941946387290955\n",
      "running_loss: 0.7326633334159851\n",
      "running_loss: 0.7839640974998474\n",
      "running_loss: 0.7463443279266357\n",
      "running_loss: 0.7895041704177856\n",
      "running_loss: 0.6661390662193298\n",
      "running_loss: 0.7815342545509338\n",
      "running_loss: 0.7346276044845581\n",
      "running_loss: 0.703798770904541\n",
      "\n",
      " loss_model: 0.6904956698417664\n",
      "\n",
      "-----epoch:4-----\n",
      "\n",
      "running_loss: 0.8083850741386414\n",
      "running_loss: 0.6037988066673279\n",
      "running_loss: 0.6163519024848938\n",
      "running_loss: 0.7858320474624634\n",
      "running_loss: 0.7537447214126587\n",
      "running_loss: 0.760384738445282\n",
      "running_loss: 0.7224587202072144\n",
      "running_loss: 0.8231813311576843\n",
      "running_loss: 0.7602992057800293\n",
      "running_loss: 0.5551387667655945\n",
      "running_loss: 0.623603105545044\n",
      "running_loss: 0.8107457160949707\n",
      "\n",
      " loss_model: 0.6353325843811035\n",
      "\n",
      "-----epoch:5-----\n",
      "\n",
      "running_loss: 0.5494516491889954\n",
      "running_loss: 0.7073372602462769\n",
      "running_loss: 0.6177973747253418\n",
      "running_loss: 0.7416114807128906\n",
      "running_loss: 0.7481217980384827\n",
      "running_loss: 0.7479199767112732\n",
      "running_loss: 0.6245488524436951\n",
      "running_loss: 0.7126691937446594\n",
      "running_loss: 0.514089822769165\n",
      "running_loss: 0.6100163459777832\n",
      "running_loss: 0.7339599132537842\n",
      "running_loss: 0.6157703995704651\n",
      "\n",
      " loss_model: 0.5434638261795044\n",
      "\n",
      "-----epoch:6-----\n",
      "\n",
      "running_loss: 0.5159056186676025\n",
      "running_loss: 0.4600845277309418\n",
      "running_loss: 0.5884782671928406\n",
      "running_loss: 0.6359386444091797\n",
      "running_loss: 0.6218421459197998\n",
      "running_loss: 0.4918372631072998\n",
      "running_loss: 0.7664414048194885\n",
      "running_loss: 0.4796344041824341\n",
      "running_loss: 0.5810059905052185\n",
      "running_loss: 0.5882488489151001\n",
      "running_loss: 0.5447592735290527\n",
      "running_loss: 0.5302590131759644\n",
      "\n",
      " loss_model: 0.43163546919822693\n",
      "\n",
      "-----epoch:7-----\n",
      "\n",
      "running_loss: 0.5900953412055969\n",
      "running_loss: 0.4240470230579376\n",
      "running_loss: 0.5178587436676025\n",
      "running_loss: 0.5638348460197449\n",
      "running_loss: 0.4563680589199066\n",
      "running_loss: 0.4664146900177002\n",
      "running_loss: 0.5547360777854919\n",
      "running_loss: 0.4525662958621979\n",
      "running_loss: 0.5052866339683533\n",
      "running_loss: 0.5029264092445374\n",
      "running_loss: 0.7179112434387207\n",
      "running_loss: 0.4629753828048706\n",
      "\n",
      " loss_model: 0.563866376876831\n",
      "\n",
      "-----epoch:8-----\n",
      "\n",
      "running_loss: 0.5160091519355774\n",
      "running_loss: 0.41396889090538025\n",
      "running_loss: 0.5392051935195923\n",
      "running_loss: 0.5731595754623413\n",
      "running_loss: 0.4488266408443451\n",
      "running_loss: 0.5721863508224487\n",
      "running_loss: 0.5120816230773926\n"
     ]
    }
   ],
   "source": [
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    Loss = []\n",
    "    print(f\"\\n-----epoch:{epoch+1}-----\\n\")\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0: \n",
    "            print(f\"running_loss: {loss.item()}\")\n",
    "      #print(inputs.shape)\n",
    "    Loss.append(loss.item())\n",
    "    print(f\"\\n loss_model: {sum(Loss)/len(Loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGRqULJjGmpb"
   },
   "outputs": [],
   "source": [
    "# load, split, shuffle test_data\n",
    "batch_size = 500\n",
    "test_set = load_data(\"test\")\n",
    "testset_x = split_data(test_set[0])\n",
    "testset_y = split_data(test_set[1])\n",
    "testset_x = shuffle_data(testset_x)\n",
    "testset_y = shuffle_data(testset_y)\n",
    "test_x = flatten_x(testset_x)\n",
    "test_y = flatten_y(testset_y)\n",
    "testset = Mydata(test_x, test_y)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka6ovc56d7HP",
    "outputId": "011adc79-371b-4baf-c90d-8e075797cf9d"
   },
   "outputs": [],
   "source": [
    "#testing accuracy \n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(testloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        out = model(inputs)\n",
    "        _, out_hat = torch.max(out , 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (out_hat == labels).sum().item()\n",
    "print(f'Accuracy of the Model: {(100 * correct / total):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gwI9z6_RfQy"
   },
   "outputs": [],
   "source": [
    "#Not bad i think, but we \n",
    "#have very low loss while accuracy isn't extremly high -> overfitting , 81% seems fine though \n",
    "#still overfiting but much better results than before, shuffeling data and reducing the cnn size helped\n",
    "#With data augmentation we could try to improve over all accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B477Amj5T88M"
   },
   "outputs": [],
   "source": [
    "#loading data for prediction, they are not labeld \n",
    "def load_data_pred():\n",
    "    images = []\n",
    "    labels = []\n",
    "    path = (\"./dataset/pred/*jpg\")\n",
    "    for file in glob.glob(path):\n",
    "        im = Image.open(file)\n",
    "        images.append(transform(im))\n",
    "        im.close()\n",
    "        labels.append(0)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "pred_set = load_data_pred()\n",
    "\n",
    "class Data_pred(Dataset):\n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.x = data[0]\n",
    "        self.y = data[1]\n",
    "        self.len = len(data[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx] , self.y[idx]\n",
    "    \n",
    "pred_data = Data_pred(pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hyr-MtWiOs46"
   },
   "outputs": [],
   "source": [
    "preddataloader = DataLoader(pred_data, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "uG1AHK-EVXB9",
    "outputId": "1f727dd3-1a42-4af2-8d23-62796e0f3e43"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels_predicted = []\n",
    "\n",
    "inputs_pred, labels_pred = next(iter(preddataloader))\n",
    "inputs_pred, labels_pred = inputs_pred.to(device), labels_pred.to(device)\n",
    "out = model(inputs_pred)\n",
    "_, out_hat = torch.max(out, 1)\n",
    "\n",
    "\n",
    "labels_predicted = [encoder[out_hat[i].item()] for i in range(len(out_hat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7ARPIx1XNat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ievr2GhUd8qW"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,5, figsize=(15, 15), sharey=True)\n",
    "\n",
    "\n",
    "idx_l = 0\n",
    "idx_h = 5\n",
    "\n",
    "for i in range(4):\n",
    "    idx_ax = 0  \n",
    "    for j in range(idx_l,idx_h):\n",
    "        inputs_pred[j-1] = inputs_pred[j-1]/2 + 0.5 #undo normalize\n",
    "        im = inputs_pred[j-1]\n",
    "        im = im.cpu().data.numpy()\n",
    "        axs[i][idx_ax].imshow(np.transpose(im, (1,2,0)))\n",
    "        axs[i][idx_ax].title.set_text(labels_predicted[j-1])\n",
    "        idx_ax+=1\n",
    "    idx_l += 5\n",
    "    idx_h += 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
